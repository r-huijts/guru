---
description:
globs:
alwaysApply: false
---
# Unsloth Implementation Guide

## Core Implementation Reference
Follow the complete implementation plan in [unsloth_llama_finetuning_plan.md](mdc:unsloth_llama_finetuning_plan.md) for step-by-step instructions.

## Unsloth Advantages
- **2x Faster Training**: Pre-optimized model weights
- **Memory Efficient**: Built-in 4-bit quantization support
- **Easy Integration**: Drop-in replacement for standard transformers
- **Optimized Models**: `unsloth/Llama-3.2-3B-Instruct` ready to use

## Key Implementation Points

### Model Loading
```python
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True
)
```

### LoRA Configuration
- **Rank**: 16 (conservative, can increase to 32 if needed)
- **Target Modules**: All attention and MLP layers
- **Alpha**: 16 (matches rank for 1.0 scaling)
- **Dropout**: 0.05 (prevent overfitting)

### Training Optimization
- **Optimizer**: `adamw_8bit` (Unsloth-optimized)
- **Precision**: FP16 for memory efficiency
- **Batch Size**: 2 with gradient accumulation
- **Checkpointing**: Save every 100 steps

## Memory Requirements
- **3B Model**: ~6GB VRAM minimum
- **1B Model**: ~4GB VRAM (fallback option)
- **System RAM**: 16GB+ recommended

## Performance Expectations
- **Training Speed**: 2-3 hours on RTX 4070
- **Memory Usage**: 6-8GB VRAM during training
- **Inference**: Fast due to Unsloth optimizations

## Troubleshooting
- **OOM Errors**: Reduce batch size, increase gradient accumulation
- **Slow Training**: Verify using Unsloth-optimized model
- **Quality Issues**: Adjust LoRA rank or learning rate

## Dataset Integration
Use [datasets/llama_optimized.jsonl](mdc:datasets/llama_optimized.jsonl) with proper prompt formatting for Llama 3.2 chat template.
